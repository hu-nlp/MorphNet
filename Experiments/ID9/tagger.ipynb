{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1801d65d1c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1801d65d1c7c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- epoch {} --- \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mscore_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_exact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"---Accuracy Set: {} Exact: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_exact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1801d65d1c7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded_all_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_gold_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding=utf8\n",
    "\n",
    "import dynet_config\n",
    "\n",
    "dynet_config.set(mem=2048, random_seed=9)\n",
    "\n",
    "import dynet as dy\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from utils import read_conll, vocab, ConllEntry\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Static variables\n",
    "EOS = '<s>'\n",
    "SW = \"start\"\n",
    "\n",
    "# HyperParameters\n",
    "LSTM_NUM_OF_LAYERS = 2\n",
    "EMBEDDINGS_SIZE = 128\n",
    "STATE_SIZE = 256\n",
    "ATTENTION_SIZE = 64\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode:\n",
    "            self.conll_train = \"/home/huseyin/Data/UD_Turkish-IMST/tr_imst-ud-train.conllu\"\n",
    "            self.conll_dev = \"/home/huseyin/Data/UD_Turkish-IMST/tr_imst-ud-dev.conllu\"\n",
    "        else:\n",
    "            self.conll_train = \"/Users/huseyinalecakir/NLP_LAB/data/tr_imst-ud-train.conllu\"\n",
    "            self.conll_dev = \"/Users/huseyinalecakir/NLP_LAB/data/tr_imst-ud-dev.conllu\"\n",
    "\n",
    "        self.c2i, self.t2i = vocab(self.conll_train)\n",
    "\n",
    "        self.i2c = {self.c2i[i]: i for i in self.c2i}\n",
    "        self.i2t = {self.t2i[i]: i for i in self.t2i}\n",
    "\n",
    "        CHAR_VOCAB_SIZE = len(self.c2i)\n",
    "        TAG_VOCAB_SIZE = len(self.t2i)\n",
    "\n",
    "        self.model = dy.Model()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_fwd_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, self.model)\n",
    "        self.enc_bwd_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, self.model)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS,\n",
    "                                       2 * STATE_SIZE + EMBEDDINGS_SIZE + STATE_SIZE * 2,\n",
    "                                       STATE_SIZE,\n",
    "                                       self.model)\n",
    "\n",
    "        # Attention\n",
    "        self.attention_w1 = self.model.add_parameters((ATTENTION_SIZE, STATE_SIZE * 2))\n",
    "        self.attention_w2 = self.model.add_parameters((ATTENTION_SIZE, STATE_SIZE * LSTM_NUM_OF_LAYERS * 2))\n",
    "        self.attention_v = self.model.add_parameters((1, ATTENTION_SIZE))\n",
    "        \n",
    "        # Attention Context\n",
    "        self.attention_w1_context = self.model.add_parameters((ATTENTION_SIZE, STATE_SIZE * 2))\n",
    "        self.attention_w2_context = self.model.add_parameters((ATTENTION_SIZE, STATE_SIZE * LSTM_NUM_OF_LAYERS * 2))\n",
    "        self.attention_v_context = self.model.add_parameters((1, ATTENTION_SIZE))\n",
    "\n",
    "        # MLP - Softmax\n",
    "        self.decoder_w = self.model.add_parameters((TAG_VOCAB_SIZE, STATE_SIZE))\n",
    "        self.decoder_b = self.model.add_parameters((TAG_VOCAB_SIZE))\n",
    "\n",
    "        # Lookups\n",
    "        self.input_lookup = self.model.add_lookup_parameters((CHAR_VOCAB_SIZE, EMBEDDINGS_SIZE))\n",
    "        self.output_lookup = self.model.add_lookup_parameters((TAG_VOCAB_SIZE, EMBEDDINGS_SIZE))\n",
    "\n",
    "    def embed_word(self, word):\n",
    "        return [self.input_lookup[char] for char in word]\n",
    "\n",
    "    def run_lstm(self, init_state, input_vecs):\n",
    "        s = init_state\n",
    "        out_vectors = []\n",
    "        for vector in input_vecs:\n",
    "            s = s.add_input(vector)\n",
    "            out_vector = s.output()\n",
    "            out_vectors.append(out_vector)\n",
    "        return out_vectors\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        word_rev = list(reversed(word))\n",
    "        fwd_vectors = self.run_lstm(self.enc_fwd_lstm.initial_state(), word)\n",
    "        bwd_vectors = self.run_lstm(self.enc_bwd_lstm.initial_state(), word_rev)\n",
    "        bwd_vectors = list(reversed(bwd_vectors))\n",
    "        vectors = [dy.concatenate(list(p)) for p in zip(fwd_vectors, bwd_vectors)]\n",
    "        return vectors\n",
    "\n",
    "    def convert2chars(self, ints):\n",
    "        return [self.i2t[i] for i in ints]\n",
    "\n",
    "    def compute_accuracy(self, gold, predicted, metric):\n",
    "        result = 0.0\n",
    "        if metric is \"set_match\":\n",
    "            correct_out = 0.0\n",
    "            for g in gold:\n",
    "                if g in predicted:\n",
    "                    correct_out += 1.0\n",
    "            result = correct_out / len(gold)\n",
    "        elif metric is \"exact_match\":\n",
    "            if len(predicted) == len(gold):\n",
    "                all_equal = True\n",
    "                for g, p in zip(gold, predicted):\n",
    "                    if g != p:\n",
    "                        all_equal = False\n",
    "                if all_equal:\n",
    "                    result = 1.0\n",
    "        else:\n",
    "            raise ValueError(\"Undefined metric.\")\n",
    "        return result\n",
    "    \n",
    "    def generate(self, encoded, word_context):\n",
    "        w = dy.parameter(self.decoder_w)\n",
    "        b = dy.parameter(self.decoder_b)\n",
    "        w1 = dy.parameter(self.attention_w1)\n",
    "        \n",
    "        w1_context = dy.parameter(self.attention_w1_context)\n",
    "\n",
    "        input_mat = dy.concatenate_cols(encoded)\n",
    "        input_context = dy.concatenate_cols(word_context)\n",
    "\n",
    "        w1dt = None\n",
    "        w1dt_context = None\n",
    "        \n",
    "        last_output_embeddings = self.output_lookup[self.t2i[EOS]]\n",
    "        s = self.dec_lstm.initial_state().add_input(dy.concatenate([dy.vecInput(STATE_SIZE * 2),\n",
    "                                                                    last_output_embeddings,\n",
    "                                                                    dy.vecInput(STATE_SIZE * 2)]))\n",
    "\n",
    "        out = []\n",
    "        count_EOS = 0\n",
    "        limit_features = 10\n",
    "        for i in range(limit_features):\n",
    "            if count_EOS == 2: break\n",
    "            # w1dt can be computed and cached once for the entire decoding phase\n",
    "            w1dt = w1dt or w1 * input_mat\n",
    "            w1dt_context = w1dt_context or w1_context * input_context\n",
    "            vector = dy.concatenate([self.attend(input_mat, s, w1dt),\n",
    "                                     last_output_embeddings,\n",
    "                                     self.attend_context(input_context, s, w1dt_context)])\n",
    "            \n",
    "            s = s.add_input(vector)\n",
    "            out_vector = w * s.output() + b\n",
    "            probs = dy.softmax(out_vector).vec_value()\n",
    "            next_char = probs.index(max(probs))\n",
    "            last_output_embeddings = self.output_lookup[next_char]\n",
    "            if self.i2t[next_char] == EOS:\n",
    "                count_EOS += 1\n",
    "                continue\n",
    "            out.append(next_char)\n",
    "        return out\n",
    "\n",
    "    def attend(self, input_mat, state, w1dt):\n",
    "        w2 = dy.parameter(self.attention_w2)\n",
    "        v = dy.parameter(self.attention_v)\n",
    "\n",
    "        # input_mat: (encoder_state x seqlen) => input vecs concatenated as cols\n",
    "        # w1dt: (attdim x seqlen)\n",
    "        # w2dt: (attdim,1)\n",
    "        w2dt = w2 * dy.concatenate(list(state.s()))\n",
    "        # att_weights: (seqlen,) row vector\n",
    "        # unnormalized: (seqlen,)\n",
    "        unnormalized = dy.transpose(v * dy.tanh(dy.colwise_add(w1dt, w2dt)))\n",
    "        att_weights = dy.softmax(unnormalized)\n",
    "        # context: (encoder_state)\n",
    "        context = input_mat * att_weights\n",
    "        return context\n",
    "    \n",
    "    def attend_context(self, input_mat, state, w1dt_context):\n",
    "        w2_context = dy.parameter(self.attention_w2_context)\n",
    "        v_context = dy.parameter(self.attention_v_context)\n",
    "\n",
    "        # input_mat: (encoder_state x seqlen) => input vecs concatenated as cols\n",
    "        # w1dt: (attdim x seqlen)\n",
    "        # w2dt: (attdim,1)\n",
    "        w2dt_context = w2_context * dy.concatenate(list(state.s()))\n",
    "        # att_weights: (seqlen,) row vector\n",
    "        # unnormalized: (seqlen,)\n",
    "        unnormalized = dy.transpose(v_context * dy.tanh(dy.colwise_add(w1dt_context, w2dt_context)))\n",
    "        att_weights = dy.softmax(unnormalized)\n",
    "        # context: (encoder_state)\n",
    "        context = input_mat * att_weights\n",
    "        return context\n",
    "\n",
    "    def decode(self, vectors, decoder_seq, word_context):\n",
    "        w = dy.parameter(self.decoder_w)\n",
    "        b = dy.parameter(self.decoder_b)\n",
    "        w1 = dy.parameter(self.attention_w1)\n",
    "        \n",
    "        w1_context = dy.parameter(self.attention_w1_context)\n",
    "        input_mat = dy.concatenate_cols(vectors)\n",
    "        input_context = dy.concatenate_cols(word_context)\n",
    "        \n",
    "        w1dt = None\n",
    "        w1dt_context = None\n",
    "        \n",
    "        last_output_embeddings = self.output_lookup[self.t2i[EOS]]\n",
    "        s = self.dec_lstm.initial_state().add_input(dy.concatenate([dy.vecInput(STATE_SIZE * 2),\n",
    "                                                                    last_output_embeddings,\n",
    "                                                                    dy.vecInput(STATE_SIZE * 2)]))\n",
    "        loss = []\n",
    "\n",
    "        for char in decoder_seq:\n",
    "            # w1dt can be computed and cached once for the entire decoding phase\n",
    "            w1dt = w1dt or w1 * input_mat\n",
    "            w1dt_context = w1dt_context or w1_context * input_context\n",
    "            vector = dy.concatenate([self.attend(input_mat, s, w1dt),\n",
    "                                     last_output_embeddings,\n",
    "                                     self.attend_context(input_context, s, w1dt_context)])\n",
    "            s = s.add_input(vector)\n",
    "            out_vector = w * s.output() + b\n",
    "            probs = dy.softmax(out_vector)\n",
    "            last_output_embeddings = self.output_lookup[char]\n",
    "            loss.append(-dy.log(dy.pick(probs, char)))\n",
    "        loss = dy.esum(loss)\n",
    "        return loss\n",
    "\n",
    "    def get_loss_entry(self, encoded, decoder_seq, word_context):\n",
    "        return self.decode(encoded, decoder_seq, word_context)\n",
    "\n",
    "    def train(self):\n",
    "        trainer = dy.AdamTrainer(self.model)\n",
    "        total_loss = 0\n",
    "        entry_count = 0\n",
    "        start = time.time()\n",
    "        with open(self.conll_train, 'r') as conllFP:\n",
    "            shuffled_data = list(read_conll(conllFP, self.c2i, self.t2i))\n",
    "            if not self.mode:\n",
    "                shuffled_data = shuffled_data[:10]\n",
    "            random.shuffle(shuffled_data)\n",
    "            for iSentence, sentence in enumerate(shuffled_data):\n",
    "                conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n",
    "                dy.renew_cg()\n",
    "                \n",
    "                context = [] \n",
    "                embedded = self.embed_word([self.c2i[SW]])\n",
    "                encoded = self.encode_word(embedded)\n",
    "                context.append(encoded[-1])\n",
    "                \n",
    "                for entry in conll_sentence:\n",
    "                    embedded = self.embed_word(entry.idChars)\n",
    "                    encoded = self.encode_word(embedded)\n",
    "                    entry.encoded_all_s = encoded\n",
    "                    entry.encoded_last_s = encoded[-1]\n",
    "                    context.append(entry.encoded_last_s)\n",
    "                    \n",
    "                for idx, entry in enumerate(conll_sentence):\n",
    "                    word_context = [c for i, c in enumerate(context) if i != idx]\n",
    "                    loss = self.get_loss_entry(entry.encoded_all_s, entry.decoder_gold_input, word_context)\n",
    "                    loss_value = loss.value()\n",
    "                    loss.backward()\n",
    "                    trainer.update()\n",
    "                    total_loss += loss_value\n",
    "                    entry_count += 1\n",
    "                    \n",
    "                if iSentence % 500 == 0:\n",
    "                    print(\n",
    "                    \"Sentence: {} Loss: {} Time: {}\".format(iSentence, total_loss / (entry_count), time.time() - start))\n",
    "                    start = time.time()\n",
    "\n",
    "    def evaluate(self):\n",
    "        count = 0\n",
    "        correct_set = 0.0\n",
    "        correct_exact = 0.0\n",
    "        start = time.time()\n",
    "        with open(self.conll_dev, 'r') as conllFP:\n",
    "            for iSentence, sentence in enumerate(read_conll(conllFP, self.c2i, self.t2i)):\n",
    "                if not self.mode:\n",
    "                    if iSentence > 2:\n",
    "                        break\n",
    "                conll_sentence = [entry for entry in sentence if isinstance(entry, ConllEntry)]\n",
    "               \n",
    "                context = [] \n",
    "                embedded = self.embed_word([self.c2i[SW]])\n",
    "                encoded = self.encode_word(embedded)\n",
    "                context.append(encoded[-1])\n",
    "                \n",
    "                for entry in conll_sentence:\n",
    "                    embedded = self.embed_word(entry.idChars)\n",
    "                    encoded = self.encode_word(embedded)\n",
    "                    entry.encoded_all_s = encoded\n",
    "                    entry.encoded_last_s = encoded[-1]\n",
    "                    context.append(entry.encoded_last_s)\n",
    "                    \n",
    "                for idx, entry in enumerate(conll_sentence):\n",
    "                    word_context = [c for i, c in enumerate(context) if i != idx]\n",
    "                    predicted_sequence = self.generate(entry.encoded_all_s, word_context)\n",
    "                    correct_set += self.compute_accuracy(entry.decoder_gold_output, predicted_sequence, \"set_match\")\n",
    "                    correct_exact += self.compute_accuracy(entry.decoder_gold_output, predicted_sequence, \"exact_match\")\n",
    "                    count += 1\n",
    "            score_set = float(correct_set) * 100 / count\n",
    "            score_exact = float(correct_exact) * 100 / count\n",
    "        print(\"Evaluation duration : {}\".format(time.time() - start))\n",
    "        return score_set, score_exact\n",
    "\n",
    "    def run(self):\n",
    "        num_epoch = 30\n",
    "        highestExactScore = 0.0\n",
    "        highestSetScore = 0.0\n",
    "        set_eId = 0\n",
    "        exact_eId = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epoch):\n",
    "            print(\"--- epoch {} --- \".format(epoch + 1))\n",
    "            self.train()\n",
    "            score_set, score_exact = self.evaluate()\n",
    "            print (\"---Accuracy Set: {} Exact: {}\".format(score_set, score_exact))\n",
    "            if score_exact >= highestExactScore:\n",
    "                highestExactScore = score_exact\n",
    "                exact_eId = epoch + 1\n",
    "            if score_set >= highestSetScore:\n",
    "                highestSetScore = score_set\n",
    "                set_eId = epoch + 1\n",
    "            print (\"Highest Exact: {} at epoch {}\".format(highestExactScore, exact_eId))\n",
    "            print (\"Highest Set: {} at epoch {}\".format(highestSetScore, set_eId))\n",
    "            print (\"Epoch: {} Total duration: {}\".format(epoch+1, time.time() - start))\n",
    "            start = time.time()\n",
    "            \n",
    "try:\n",
    "    if len(sys.argv) > 1:\n",
    "        experiment_mode = True if int(sys.argv[1]) is 1 else False\n",
    "    else:\n",
    "        experiment_mode = False\n",
    "except ValueError:\n",
    "    experiment_mode = False\n",
    "\n",
    "learner = Learner(experiment_mode)\n",
    "learner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
