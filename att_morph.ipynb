{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import dynet as dy\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "dyparams = dy.DynetParams()\n",
    "dyparams.from_args()\n",
    "dyparams.set_mem(4096)\n",
    "dyparams.set_random_seed(1)\n",
    "dyparams.init()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "conll_train = \"/Users/huseyinalecakir/NLP_LAB/data/tr_imst-ud-train.conllu\"\n",
    "conll_dev = \"/Users/huseyinalecakir/NLP_LAB/data/tr_imst-ud-dev.conllu\"\n",
    "\n",
    "c2i, w2i, features = utils.vocab(conll_train)\n",
    "\n",
    "EOS = '<s>'\n",
    "\n",
    "int2char = {c2i[i] : i for i in c2i}\n",
    "char2int = c2i\n",
    "\n",
    "VOCAB_SIZE = len(c2i)\n",
    "\n",
    "LSTM_NUM_OF_LAYERS = 2\n",
    "EMBEDDINGS_SIZE = 128\n",
    "STATE_SIZE = 256\n",
    "ATTENTION_SIZE = 64\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "enc_fwd_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, model)\n",
    "enc_bwd_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, model)\n",
    "\n",
    "dec_lstm = dy.LSTMBuilder(LSTM_NUM_OF_LAYERS, STATE_SIZE*2+EMBEDDINGS_SIZE, STATE_SIZE, model)\n",
    "\n",
    "input_lookup = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDINGS_SIZE))\n",
    "attention_w1 = model.add_parameters( (ATTENTION_SIZE, STATE_SIZE*2))\n",
    "attention_w2 = model.add_parameters( (ATTENTION_SIZE, STATE_SIZE*LSTM_NUM_OF_LAYERS*2))\n",
    "attention_v = model.add_parameters( (1, ATTENTION_SIZE))\n",
    "decoder_w = model.add_parameters( (VOCAB_SIZE, STATE_SIZE))\n",
    "decoder_b = model.add_parameters( (VOCAB_SIZE))\n",
    "output_lookup = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDINGS_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    sentence = [\"<s>\"] + list(sentence) + [\"<s>\"]\n",
    "    sentence = [char2int[c] for c in sentence]\n",
    "\n",
    "    global input_lookup\n",
    "\n",
    "    return [input_lookup[char] for char in sentence]\n",
    "\n",
    "def run_lstm(init_state, input_vecs):\n",
    "    s = init_state\n",
    "\n",
    "    out_vectors = []\n",
    "    for vector in input_vecs:\n",
    "        s = s.add_input(vector)\n",
    "        out_vector = s.output()\n",
    "        out_vectors.append(out_vector)\n",
    "    return out_vectors\n",
    "\n",
    "def encode_sentence(enc_fwd_lstm, enc_bwd_lstm, sentence):\n",
    "    sentence_rev = list(reversed(sentence))\n",
    "\n",
    "    fwd_vectors = run_lstm(enc_fwd_lstm.initial_state(), sentence)\n",
    "    bwd_vectors = run_lstm(enc_bwd_lstm.initial_state(), sentence_rev)\n",
    "    bwd_vectors = list(reversed(bwd_vectors))\n",
    "    vectors = [dy.concatenate(list(p)) for p in zip(fwd_vectors, bwd_vectors)]\n",
    "\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def attend(input_mat, state, w1dt):\n",
    "    global attention_w2\n",
    "    global attention_v\n",
    "    w2 = dy.parameter(attention_w2)\n",
    "    v = dy.parameter(attention_v)\n",
    "\n",
    "    # input_mat: (encoder_state x seqlen) => input vecs concatenated as cols\n",
    "    # w1dt: (attdim x seqlen)\n",
    "    # w2dt: (attdim x attdim)\n",
    "    w2dt = w2*dy.concatenate(list(state.s()))\n",
    "    # att_weights: (seqlen,) row vector\n",
    "    unnormalized = dy.transpose(v * dy.tanh(dy.colwise_add(w1dt, w2dt)))\n",
    "    att_weights = dy.softmax(unnormalized)\n",
    "    # context: (encoder_state)\n",
    "    context = input_mat * att_weights\n",
    "    return context\n",
    "\n",
    "\n",
    "def decode(dec_lstm, vectors, output):\n",
    "    #output = [EOS] + list(output) + [EOS]\n",
    "    #output = [char2int[c] for c in output]\n",
    "    output = [c2i[\"<s>\"]] + output + [c2i[\"<s>\"]]\n",
    "\n",
    "    w = dy.parameter(decoder_w)\n",
    "    b = dy.parameter(decoder_b)\n",
    "    w1 = dy.parameter(attention_w1)\n",
    "    input_mat = dy.concatenate_cols(vectors)\n",
    "    w1dt = None\n",
    "\n",
    "    last_output_embeddings = output_lookup[c2i[\"<s>\"]]\n",
    "    s = dec_lstm.initial_state().add_input(dy.concatenate([dy.vecInput(STATE_SIZE*2), last_output_embeddings]))\n",
    "    loss = []\n",
    "\n",
    "    for char in output:\n",
    "        # w1dt can be computed and cached once for the entire decoding phase\n",
    "        w1dt = w1dt or w1 * input_mat\n",
    "        vector = dy.concatenate([attend(input_mat, s, w1dt), last_output_embeddings])\n",
    "        s = s.add_input(vector)\n",
    "        out_vector = w * s.output() + b\n",
    "        probs = dy.softmax(out_vector)\n",
    "        last_output_embeddings = output_lookup[char]\n",
    "        loss.append(-dy.log(dy.pick(probs, char)))\n",
    "    loss = dy.esum(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def generate(in_seq, enc_fwd_lstm, enc_bwd_lstm, dec_lstm):\n",
    "    embedded = embed_sentence(in_seq)\n",
    "    encoded = encode_sentence(enc_fwd_lstm, enc_bwd_lstm, embedded)\n",
    "\n",
    "    w = dy.parameter(decoder_w)\n",
    "    b = dy.parameter(decoder_b)\n",
    "    w1 = dy.parameter(attention_w1)\n",
    "    input_mat = dy.concatenate_cols(encoded)\n",
    "    w1dt = None\n",
    "\n",
    "    last_output_embeddings = output_lookup[c2i[\"<s>\"]]\n",
    "    s = dec_lstm.initial_state().add_input(dy.concatenate([dy.vecInput(STATE_SIZE * 2), last_output_embeddings]))\n",
    "\n",
    "    out = []\n",
    "    count_EOS = 0\n",
    "    for i in range(len(in_seq)*2):\n",
    "        if count_EOS == 2: break\n",
    "        # w1dt can be computed and cached once for the entire decoding phase\n",
    "        w1dt = w1dt or w1 * input_mat\n",
    "        vector = dy.concatenate([attend(input_mat, s, w1dt), last_output_embeddings])\n",
    "        s = s.add_input(vector)\n",
    "        out_vector = w * s.output() + b\n",
    "        probs = dy.softmax(out_vector).vec_value()\n",
    "        next_char = probs.index(max(probs))\n",
    "        last_output_embeddings = output_lookup[next_char]\n",
    "        if int2char[next_char] == EOS:\n",
    "            count_EOS += 1\n",
    "            continue\n",
    "\n",
    "        out.append(next_char)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_loss(input_sentence, output_sentence, enc_fwd_lstm, enc_bwd_lstm, dec_lstm):\n",
    "    dy.renew_cg()\n",
    "    embedded = embed_sentence(input_sentence)\n",
    "    encoded = encode_sentence(enc_fwd_lstm, enc_bwd_lstm, embedded)\n",
    "    return decode(dec_lstm, encoded, output_sentence)\n",
    "\n",
    "def convert2chars(ints, i2c):\n",
    "    return [i2c[i] for i in ints]\n",
    "\n",
    "def compute_accuracy(gold, predicted, metric=\"set\"):\n",
    "    result = 0.0\n",
    "    if metric is \"set_match\":\n",
    "        correct_out = 0.0\n",
    "        for g in gold:\n",
    "            if gold in predicted:\n",
    "                correct_out += 1\n",
    "        result += correct_out/len(gold)\n",
    "    elif metric is \"exact_match\":\n",
    "        if len(predicted) == len(gold):\n",
    "            all_equal = True\n",
    "            for g, p in zip(gold, predicted):\n",
    "                if g != p:\n",
    "                    all_equal = False\n",
    "            if all_equal:\n",
    "                result = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, conll_path):\n",
    "    trainer = dy.AdamTrainer(model)\n",
    "    total_loss = 0\n",
    "    entry_count = 0\n",
    "    start = time.time()\n",
    "    with open(conll_path, 'r') as conllFP:\n",
    "        shuffled_data = list(read_conll(conllFP, c2i, w2i))\n",
    "        shuffled_data = shuffled_data\n",
    "        random.shuffle(shuffled_data)\n",
    "        for iSentence, sentence in enumerate(shuffled_data):\n",
    "            conll_sentence = [entry for entry in sentence if isinstance(entry, utils.ConllEntry)]\n",
    "            for entry in conll_sentence:\n",
    "                loss = get_loss(entry.chars, entry.decoder_gold_output, enc_fwd_lstm, enc_bwd_lstm, dec_lstm)\n",
    "                loss_value = loss.value()\n",
    "                loss.backward()\n",
    "                trainer.update()\n",
    "                total_loss += loss_value\n",
    "                entry_count += 1\n",
    "            if iSentence % 500 == 0:\n",
    "                print(\"Sentence: {} Loss: {} Time: {}\".format(iSentence, total_loss/(entry_count), time.time() - start))\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model,conll_path):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with open(conll_path, 'r') as conllFP:\n",
    "        for iSentence, sentence in enumerate(read_conll(conllFP, c2i, w2i)):\n",
    "            conll_sentence = [entry for entry in sentence if isinstance(entry, utils.ConllEntry)]\n",
    "            for entry in conll_sentence:\n",
    "                predicted_sequence = generate(entry.chars, enc_fwd_lstm, enc_bwd_lstm, dec_lstm)\n",
    "                correct = compute_accuracy(entry.decoder_gold_output, predicted_sequence, \"exact_match\")\n",
    "                count += 1\n",
    "        score = float(correct) * 100 / count\n",
    "    print(\"Evaluation duration : {}\".format(time.time()-start))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 3\n",
    "highestScore = 0.0\n",
    "eId = 0\n",
    "start = time.time()\n",
    "for epoch in range(num_epoch):\n",
    "    print(\"--- epoch {} --- \".format(epoch))\n",
    "    train(model, conll_train)\n",
    "    score = evaluate(model, conll_dev)\n",
    "    print \"---\\nAccuracy:\\t%.2f\" % score\n",
    "    if score >= highestScore:\n",
    "        highestScore = score\n",
    "        eId = epoch + 1\n",
    "    print \"Highest: %.2f at epoch %d\" % (highestScore, eId)\n",
    "    print(\"Epoch: {} Total duration: {}\".format(epoch, time.time()-start))\n",
    "    start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and dy.renew_cg() in generate function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
